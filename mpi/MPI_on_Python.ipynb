{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7236f31d",
   "metadata": {},
   "source": [
    "\n",
    "# Message passing interface\n",
    "\n",
    "- MPI is an application programming interface (API) for communication\n",
    "  between separate processes\n",
    "- MPI programs are portable and scalable\n",
    "    - the same program can run on different types of computers, from PC's\n",
    "      to supercomputers\n",
    "    - the most widely used approach for distributed parallel computing\n",
    "- MPI is flexible and comprehensive\n",
    "    - large (over 300 procedures)\n",
    "    - concise (often only 6 procedures are needed)\n",
    "- MPI standard defines C and Fortran interfaces\n",
    "    - MPI for Python (mpi4py) provides an unofficial Python interface\n",
    "\n",
    "\n",
    "# Processes and threads\n",
    "\n",
    "![](../docs/img/processes-threads-highlight-proc.svg){.center width=80%}\n",
    "\n",
    "\n",
    "<div class=\"column\">\n",
    "\n",
    "## Process\n",
    "\n",
    "- Independent execution units\n",
    "- Have their own state information and *own memory* address space\n",
    "\n",
    "</div>\n",
    "<div class=\"column\">\n",
    "\n",
    "## Thread\n",
    "\n",
    "- A single process may contain multiple threads\n",
    "- Have their own state information, but *share* the *same memory*\n",
    "  address space\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "# Execution model\n",
    "\n",
    "- MPI program is launched as a set of *independent*, *identical processes*\n",
    "    - execute the same program code and instructions\n",
    "    - can reside in different nodes (or even in different computers)\n",
    "- The way to launch a MPI program depends on the system\n",
    "    - mpiexec, mpirun, srun, aprun, ...\n",
    "    - mpiexec/mpirun in training class\n",
    "    - srun on puhti.csc.fi\n",
    "\n",
    "\n",
    "# MPI rank\n",
    "\n",
    "- Rank: ID number given to a process\n",
    "    - it is possible to query for rank\n",
    "    - processes can perform different tasks based on their rank\n",
    "\n",
    "```python\n",
    "if (rank == 0):\n",
    "    # do something\n",
    "elif (rank == 1):\n",
    "    # do something else\n",
    "else:\n",
    "    # all other processes do something different\n",
    "```\n",
    "\n",
    "\n",
    "# Data model\n",
    "\n",
    "- Each MPI process has its own *separate* memory space, i.e. all\n",
    "  variables and data structures are *local* to the process\n",
    "- Processes can exchange data by sending and receiving messages\n",
    "\n",
    "![](../docs/img/data-model.svg){.center width=90%}\n",
    "\n",
    "\n",
    "# MPI communicator\n",
    "\n",
    "- Communicator: a group containing all the processes that will participate\n",
    "  in communication\n",
    "    - in mpi4py most MPI calls are implemented as methods of a\n",
    "    communicator object\n",
    "    - `MPI_COMM_WORLD` contains all processes (`MPI.COMM_WORLD` in\n",
    "    mpi4py)\n",
    "    - user can define custom communicators\n",
    "\n",
    "\n",
    "# Routines in MPI for Python\n",
    "\n",
    "- Communication between processes\n",
    "    - sending and receiving messages between two processes\n",
    "    - sending and receiving messages between several processes\n",
    "- Synchronization between processes\n",
    "- Communicator creation and manipulation\n",
    "- Advanced features (e.g. user defined datatypes, one-sided communication\n",
    "  and parallel I/O)\n",
    "\n",
    "\n",
    "# Getting started\n",
    "\n",
    "- Basic methods of communicator object\n",
    "    - `Get_size()` Number of processes in communicator\n",
    "    - `Get_rank()` rank of this process\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD # communicator object containing all processes\n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(\"I am rank %d in group of %d processes\" % (rank, size))\n",
    "```\n",
    "\n",
    "\n",
    "# Running an example program\n",
    "\n",
    "```bash\n",
    "$ mpiexec -n 4 python3 hello.py\n",
    "\n",
    "I am rank 2 in group of 4 processes\n",
    "I am rank 0 in group of 4 processes\n",
    "I am rank 3 in group of 4 processes\n",
    "I am rank 1 in group of 4 processes\n",
    "```\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD # communicator object containing all processes\n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(\"I am rank %d in group of %d processes\" % (rank, size))\n",
    "```\n",
    "\n",
    "\n",
    "# Point-to-Point Communication\n",
    "\n",
    "\n",
    "<div class=\"column\">\n",
    "\n",
    "- Data is local to the MPI processes\n",
    "    - They need to *communicate* to coordinate work\n",
    "- Point-to-point communication\n",
    "    - Messages are sent between two processes\n",
    "- Collective communication\n",
    "    - Involving a number of processes at the same time\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"column\">\n",
    "\n",
    "![](../docs/img/communication-schematic.svg){.center width=50%}\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "# MPI point-to-point operations\n",
    "\n",
    "- One process *sends* a message to another process that *receives* it\n",
    "- Sends and receives in a program should match - one receive per send\n",
    "- Each message contains\n",
    "    - The actual *data* that is to be sent\n",
    "    - The *datatype* of each element of data\n",
    "    - The *number of elements* the data consists of\n",
    "    - An identification number for the message (*tag*)\n",
    "    - The ranks of the *source* and *destination* process\n",
    "- With **mpi4py** it is often enough to specify only *data* and\n",
    "  *source* and *destination*\n",
    "\n",
    "# Sending and receiving data\n",
    "\n",
    "- Sending and receiving a dictionary\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD # communicator object containing all processes\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "```\n",
    "\n",
    "\n",
    "# Sending and receiving data\n",
    "\n",
    "- Arbitrary Python objects can be communicated with the send and\n",
    "  receive methods of a communicator\n",
    "\n",
    "<div class=\"column\">\n",
    "\n",
    "`.send(data, dest)`\n",
    "  : `data`{.input}\n",
    "    : Python object to send\n",
    "\n",
    "    `dest`{.input}\n",
    "    : destination rank\n",
    "\n",
    "</div>\n",
    "<div class=\"column\">\n",
    "\n",
    "`.recv(source)`\n",
    "  : `source`{.input}\n",
    "    : source rank\n",
    "    : note: data is provided as return value\n",
    "\n",
    "</div>\n",
    "\n",
    "- Destination and source ranks have to match!\n",
    "\n",
    "\n",
    "# Blocking routines & deadlocks\n",
    "\n",
    "- `send()` and `recv()` are *blocking* routines\n",
    "    - the functions exit only once it is safe to use the data (memory)\n",
    "    involved in the communication\n",
    "- Completion depends on other processes => risk for *deadlocks*\n",
    "    - for example, if all processes call `recv()` there is no-one left to\n",
    "    call a corresponding `send()` and the program is *stuck forever*\n",
    "\n",
    "\n",
    "# Typical point-to-point communication patterns\n",
    "\n",
    "![](../docs/img/comm_patt.svg){.center width=100%}\n",
    "\n",
    "<br>\n",
    "\n",
    "- Incorrect ordering of sends and receives may result in a deadlock\n",
    "\n",
    "\n",
    "# Case study: parallel sum\n",
    "\n",
    "<div class=column style=\"width:30%\">\n",
    "![](img/parallel-sum-0.svg){.center width=70%}\n",
    "</div>\n",
    "\n",
    "<div class=column style=\"width:68%\">\n",
    "## Initial state\n",
    "\n",
    "An array A containing floating point numbers read from a a file by the first\n",
    "MPI task (rank 0).\n",
    "\n",
    "## Goal\n",
    "\n",
    "Calculate the total sum of all elements in array A in parallel.\n",
    "</div>\n",
    "\n",
    "\n",
    "# Case study: parallel sum\n",
    "\n",
    "<div class=column style=\"width:30%\">\n",
    "![](../docs/img/parallel-sum-0.svg){.center width=70%}\n",
    "</div>\n",
    "\n",
    "<div class=column style=\"width:68%\">\n",
    "## Parallel algorithm\n",
    "\n",
    "<pre style=\"border:none; margin-top:1em; font-size:1em\">\n",
    "1. Scatter the data\n",
    "   1.1. receive operation for scatter\n",
    "   1.2. send operation for scatter\n",
    "2. Compute partial sums in parallel\n",
    "3. Gather the partial sums\n",
    "   3.1. receive operation for gather\n",
    "   3.2. send operation for gather\n",
    "4. Compute the total sum\n",
    "</pre>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "# Step 1.1: Receive operation for scatter\n",
    "\n",
    "![](../docs/img/parallel-sum-1.1.png){.center width=55%}\n",
    "\n",
    "\n",
    "# Step 1.2: Send operation for scatter\n",
    "\n",
    "![](../docs/img/parallel-sum-1.2.png){.center width=55%}\n",
    "\n",
    "\n",
    "# Step 2: Compute partial sums in parallel\n",
    "\n",
    "![](../docs/img/parallel-sum-2.png){.center width=55%}\n",
    "\n",
    "\n",
    "# Step 3.1: Receive operation for gather\n",
    "\n",
    "![](../docs/img/parallel-sum-3.1.png){.center width=55%}\n",
    "\n",
    "\n",
    "# Step 3.2: Send operation for gather\n",
    "\n",
    "![](../docs/img/parallel-sum-3.2.png){.center width=55%}\n",
    "\n",
    "\n",
    "# Step 4: Compute the total sum\n",
    "\n",
    "![](../docs/img/parallel-sum-4.png){.center width=55%}\n",
    "\n",
    "\n",
    "# Communicating NumPy arrays\n",
    "\n",
    "- Arbitrary Python objects are converted to byte streams (pickled) when\n",
    "  sending and back to Python objects (unpickled) when receiving\n",
    "    - these conversions may be a serious overhead to communication\n",
    "- Contiguous memory buffers (such as NumPy arrays) can be communicated\n",
    "  with very little overhead using upper case methods:\n",
    "    - `Send(data, dest)`\n",
    "    - `Recv(data, source)`\n",
    "    - note the difference in receiving: the data array has to exist at the\n",
    "      time of call\n",
    "\n",
    "\n",
    "# Send/receive a NumPy array\n",
    "\n",
    "- Note the difference between upper/lower case!\n",
    "    - send/recv: general Python objects, slow\n",
    "    - Send/Recv: continuous arrays, fast\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "data = numpy.empty(100, dtype=float)\n",
    "if rank == 0:\n",
    "    data[:] = numpy.arange(100, dtype=float)\n",
    "    comm.Send(data, dest=1)\n",
    "elif rank == 1:\n",
    "    comm.Recv(data, source=0)\n",
    "```\n",
    "\n",
    "\n",
    "# Combined send and receive\n",
    "\n",
    "- Send one message and receive another with a single command\n",
    "    - reduces risk for deadlocks\n",
    "- Destination and source ranks can be same or different\n",
    "    - `MPI.PROC_NULL` can be used for *no destination/source*\n",
    "\n",
    "```python\n",
    "data = numpy.arange(10, dtype=float) * (rank + 1)\n",
    "buffer = numpy.empty(data.shape, dtype=data.dtype)\n",
    "\n",
    "if rank == 0:\n",
    "    dest, source = 1, 1\n",
    "elif rank == 1:\n",
    "    dest, source = 0, 0\n",
    "\n",
    "comm.Sendrecv(data, dest=dest, recvbuf=buffer, source=source)\n",
    "```\n",
    "\n",
    "\n",
    "# MPI datatypes\n",
    "\n",
    "- MPI has a number of predefined datatypes to represent data\n",
    "    - e.g. `MPI.INT` for integer and `MPI.DOUBLE` for float\n",
    "- No need to specify the datatype for Python objects or Numpy arrays\n",
    "    - objects are serialised as byte streams\n",
    "    - automatic detection for NumPy arrays\n",
    "- If needed, one can also define custom datatypes\n",
    "    - for example to use non-contiguous data buffers\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Point-to-point communication = messages are sent between two MPI\n",
    "  processes\n",
    "- Point-to-point operations enable any parallel communication pattern (in\n",
    "  principle)\n",
    "- Arbitrary Python objects (that can be pickled!)\n",
    "    - `send` / `recv`\n",
    "    - `sendrecv`\n",
    "- Memory buffers such as Numpy arrays\n",
    "    - `Send` / `Recv`\n",
    "    - `Sendrecv`\n",
    "\n",
    "\n",
    "\n",
    "# Non-blocking communication\n",
    "\n",
    "- Non-blocking sends and receives\n",
    "    - `isend` & `irecv`\n",
    "    - returns immediately and sends/receives in background\n",
    "    - return value is a Request object\n",
    "- Enables some computing concurrently with communication\n",
    "- Avoids many common dead-lock situations\n",
    "\n",
    "\n",
    "# Non-blocking communication\n",
    "\n",
    "- Have to finalize send/receive operations\n",
    "    - `wait()`\n",
    "        - Waits for the communication started with `isend` or `irecv` to\n",
    "          finish (blocking)\n",
    "    - `test()`\n",
    "        - Tests if the communication has finished (non-blocking)\n",
    "- You can mix non-blocking and blocking p2p routines\n",
    "    - e.g., receive `isend` with `recv`\n",
    "\n",
    "\n",
    "# Example: non-blocking send/receive\n",
    "\n",
    "```python\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = arange(size, dtype=float) * (rank + 1)\n",
    "    req = comm.Isend(data, dest=1)    # start a send\n",
    "    calculate_something(rank)         # .. do something else ..\n",
    "    req.wait()                        # wait for send to finish\n",
    "    # safe to read/write data again\n",
    "\n",
    "elif rank == 1:\n",
    "    data = empty(size, float)\n",
    "    req = comm.Irecv(data, source=0)  # post a receive\n",
    "    calculate_something(rank)         # .. do something else ..\n",
    "    req.wait()                        # wait for receive to finish\n",
    "    # data is now ready for use\n",
    "```\n",
    "\n",
    "\n",
    "# Multiple non-blocking operations\n",
    "\n",
    "- Methods `waitall()` and `waitany()` may come handy when dealing with\n",
    "  multiple non-blocking operations (available in the `MPI.Request` class)\n",
    "    - `Request.waitall(requests)`\n",
    "        - wait for all initiated requests to complete\n",
    "    - `Request.waitany(requests)`\n",
    "        - wait for any initiated request to complete\n",
    "- For example, assuming `requests` is a list of request objects, one can wait\n",
    "  for all of them to be finished with:\n",
    "\n",
    "~~~python\n",
    "MPI.Request.waitall(requests)\n",
    "~~~\n",
    "\n",
    "\n",
    "# Example: non-blocking message chain\n",
    "\n",
    "<small>\n",
    "\n",
    "~~~python\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "data = numpy.arange(10, dtype=float) * (rank + 1)  # send buffer\n",
    "buffer = numpy.zeros(10, dtype=float)              # receive buffer\n",
    "\n",
    "tgt = rank + 1\n",
    "src = rank - 1\n",
    "if rank == 0:\n",
    "    src = MPI.PROC_NULL\n",
    "if rank == size - 1:\n",
    "    tgt = MPI.PROC_NULL\n",
    "\n",
    "req = []\n",
    "req.append(comm.Isend(data, dest=tgt))\n",
    "req.append(comm.Irecv(buffer, source=src))\n",
    "\n",
    "MPI.Request.waitall(req)\n",
    "~~~\n",
    "\n",
    "</small>\n",
    "\n",
    "\n",
    "# Overlapping computation and communication\n",
    "\n",
    "<div class=\"column\">\n",
    "~~~python\n",
    "request_in = comm.Irecv(ghost_data)\n",
    "request_out = comm.Isend(border_data)\n",
    "\n",
    "compute(ghost_independent_data)\n",
    "request_in.wait()\n",
    "\n",
    "compute(border_data)\n",
    "request_out.wait()\n",
    "~~~\n",
    "</div>\n",
    "\n",
    "<div class=\"column\">\n",
    "![](../docs/img/non-blocking-pattern.png)\n",
    "</div>\n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Non-blocking communication is usually the smart way to do point-to-point\n",
    "  communication in MPI\n",
    "- Non-blocking communication realization\n",
    "    - `isend` / `Isend`\n",
    "    - `irecv` / `Irecv`\n",
    "    - `request.wait()`\n",
    "\n",
    "\n",
    "\n",
    "# Communicators\n",
    "\n",
    "- The communicator determines the \"communication universe\"\n",
    "    - The source and destination of a message is identified by process rank\n",
    "      *within* the communicator\n",
    "- So far: `MPI.COMM_WORLD`\n",
    "- Processes can be divided into subcommunicators\n",
    "    - Task level parallelism with process groups performing separate tasks\n",
    "    - Collective communication within a group of processes\n",
    "    - Parallel I/O\n",
    "\n",
    "\n",
    "# Communicators\n",
    "\n",
    "<div class=\"column\">\n",
    "- Communicators are dynamic\n",
    "- A task can belong simultaneously to several communicators\n",
    "    - Unique rank in each communicator\n",
    "</div>\n",
    "<div class=\"column\">\n",
    "![](../docs/img/communicator.svg){.center width=80%}\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "# User-defined communicators\n",
    "\n",
    "- By default a single, universal communicator exists to which all\n",
    "  processes belong (`MPI.COMM_WORLD`)\n",
    "- One can create new communicators, e.g. by splitting this into\n",
    "  sub-groups\n",
    "\n",
    "```python\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "color = rank % 4\n",
    "\n",
    "local_comm = comm.Split(color)\n",
    "local_rank = local_comm.Get_rank()\n",
    "\n",
    "print(\"Global rank: %d Local rank: %d\" % (rank, local_rank))\n",
    "```\n",
    "\n",
    "\n",
    "# Collective communication\n",
    "\n",
    "- Collective communication transmits data among all processes in a process\n",
    "  group (communicator)\n",
    "    - these routines must be called by all the processes in the group\n",
    "    - amount of sent and received data must match\n",
    "- Collective communication includes\n",
    "    - data movement\n",
    "    - collective computation\n",
    "    - synchronization\n",
    "- Example\n",
    "    - `comm.barrier()` makes every task hold until all tasks in the\n",
    "      communicator `comm` have called it\n",
    "\n",
    "\n",
    "# Collective communication\n",
    "\n",
    "- Collective communication typically outperforms point-to-point\n",
    "  communication\n",
    "- Code becomes more compact (and efficient!) and easier to maintain:\n",
    "    - For example, communicating a Numpy array of 1M elements from task 0 to all\n",
    "      other tasks:\n",
    "\n",
    "<div class=\"column\">\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "    for i in range(1, size):\n",
    "    comm.Send(data, i)\n",
    "else:\n",
    "    comm.Recv(data, 0)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column\">\n",
    "\n",
    "```python\n",
    "comm.Bcast(data, 0)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "# Broadcast\n",
    "\n",
    "- Send the same data from one process to all the other\n",
    "\n",
    "![](../docs/img/mpi-bcast.svg){.center width=80%}\n",
    "\n",
    "\n",
    "# Broadcast\n",
    "\n",
    "- Broadcast sends same data to all processes\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    py_data = {'key1' : 0.0, 'key2' : 11}  # Python object\n",
    "    data = np.arange(8) / 10.              # NumPy array\n",
    "else:\n",
    "    py_data = None\n",
    "    data = np.zeros(8)\n",
    "\n",
    "new_data = comm.bcast(py_data, root=0)\n",
    "\n",
    "comm.Bcast(data, root=0)\n",
    "```\n",
    "\n",
    "\n",
    "# Scatter\n",
    "\n",
    "- Send equal amount of data from one process to others\n",
    "- Segments A, B, ... may contain multiple elements\n",
    "\n",
    "![](../docs/img/mpi-scatter.svg){.center width=80%}\n",
    "\n",
    "\n",
    "# Scatter\n",
    "\n",
    "- Scatter distributes data to processes\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "from numpy import arange, empty\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "if rank == 0:\n",
    "    py_data = range(size)\n",
    "    data = arange(size**2, dtype=float)\n",
    "else:\n",
    "    py_data = None\n",
    "    data = None\n",
    "\n",
    "new_data = comm.scatter(py_data, root=0)  # returns the value\n",
    "\n",
    "buffer = empty(size, float)         # prepare a receive buffer\n",
    "comm.Scatter(data, buffer, root=0)  # in-place modification\n",
    "```\n",
    "\n",
    "\n",
    "# Gather\n",
    "\n",
    "- Collect data from all the process to one process\n",
    "- Segments A, B, ... may contain multiple elements\n",
    "\n",
    "![](../docs/img/mpi-gather.svg){.center width=80%}\n",
    "\n",
    "\n",
    "# Gather\n",
    "\n",
    "- Gather pulls data from all processes\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "from numpy import arange, zeros\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "data = arange(10, dtype=float) * (rank + 1)\n",
    "buffer = zeros(size * 10, float)\n",
    "\n",
    "n = comm.gather(rank, root=0)     # returns the value\n",
    "comm.Gather(data, buffer, root=0) # in-place modification\n",
    "```\n",
    "\n",
    "\n",
    "# Reduce\n",
    "\n",
    "- Applies an operation over set of processes and places result in\n",
    "  single process\n",
    "\n",
    "![](../docs/img/mpi-reduce.svg){.center width=80%}\n",
    "\n",
    "# Reduce\n",
    "\n",
    "- Reduce gathers data and applies an operation on it\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "from numpy import arange, empty\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "data = arange(10 * size, dtype=float) * (rank + 1)\n",
    "buffer = zeros(size * 10, float)\n",
    "\n",
    "n = comm.reduce(rank, op=MPI.SUM, root=0)     # returns the value\n",
    "comm.Reduce(data, buffer, op=MPI.SUM, root=0) # in-place modification\n",
    "```\n",
    "\n",
    "\n",
    "# Other common collective operations\n",
    "\n",
    "Scatterv\n",
    "  : each process receives different amount of data\n",
    "\n",
    "Gatherv\n",
    "  : each process sends different amount of data\n",
    "\n",
    "Allreduce\n",
    "  : all processes receive the results of reduction\n",
    "\n",
    "Alltoall\n",
    "  : each process sends and receives to/from each other\n",
    "\n",
    "Alltoallv\n",
    "  : each process sends and receives different amount of data\n",
    "\n",
    "\n",
    "\n",
    "# Non-blocking collectives\n",
    "\n",
    "- New in MPI 3: no support in mpi4py\n",
    "- Non-blocking collectives enable the overlapping of communication and\n",
    "  computation together with the benefits of collective communication\n",
    "- Restrictions\n",
    "    - have to be called in same order by all ranks in a communicator\n",
    "    - mixing of blocking and non-blocking collectives is not allowed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Common mistakes with collectives\n",
    "\n",
    "1. Using a collective operation within one branch of an if-else test based on\n",
    "   the rank of the process\n",
    "    - for example: `if rank == 0: comm.bcast(...)`\n",
    "    - all processes in a communicator must call a collective routine!\n",
    "2. Assuming that all processes making a collective call would complete at\n",
    "   the same time.\n",
    "3. Using the input buffer also as an output buffer:\n",
    "    - for example: `comm.Scatter(a, a, MPI.SUM)`\n",
    "    - always use different memory locations (arrays) for input and output!\n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Collective communications involve all the processes within a\n",
    "  communicator\n",
    "    - all processes must call them\n",
    "- Collective operations make code more transparent and compact\n",
    "- Collective routines allow optimizations by MPI library\n",
    "- MPI-3 contains also non-blocking collectives, but these are currently\n",
    "  not supported by MPI for Python\n",
    "\n",
    "\n",
    "# On-line resources\n",
    "\n",
    "- Documentation for mpi4py is quite limited\n",
    "    - short on-line manual available at\n",
    "    [https://mpi4py.readthedocs.io/](https://mpi4py.readthedocs.io/)\n",
    "- Some good references:\n",
    "    - \"A Python Introduction to Parallel Programming with MPI\" *by Jeremy\n",
    "      Bejarano* [http://materials.jeremybejarano.com/MPIwithPython/](http://materials.jeremybejarano.com/MPIwithPython/)\n",
    "    - \"mpi4py examples\" *by Jörg Bornschein* [https://github.com/jbornschein/mpi4py-examples](https://github.com/jbornschein/mpi4py-examples)\n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "- mpi4py provides Python interface to MPI\n",
    "- MPI calls via communicator object\n",
    "- Possible to communicate arbitrary Python objects\n",
    "- NumPy arrays can be communicated with nearly same speed as in C/Fortran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15b4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
